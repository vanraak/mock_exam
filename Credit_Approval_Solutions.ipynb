{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YM-B32GUf5EA"
   },
   "source": [
    "# Credit Approval\n",
    "\n",
    "This file concerns credit card applications.  All attribute names and values have been changed to meaningless symbols to protect confidentiality of the data.\n",
    "\n",
    "The dataset contains the following variables:\n",
    "\n",
    "approval:\tTarget\tCategorical<br>\n",
    "A15:\tFeature,\tContinuous<br>\n",
    "A14:\tFeature,\tContinuous<br>\n",
    "A13:\tFeature,\tCategorical<br>\n",
    "A12:\tFeature,\tCategorical<br>\n",
    "A11:\tFeature,\tContinuous<br>\n",
    "A10:\tFeature,\tCategorical<br>\n",
    "A9:\tFeature,\tCategorical<br>\n",
    "A8:\tFeature,\tContinuous<br>\n",
    "A7:\tFeature,\tCategorical<br>\n",
    "A6:\tFeature,\tCategorical<br>\n",
    "A5:\tFeature,\tCategorical<br>\n",
    "A4:\tFeature,\tCategorical<br>\n",
    "A3:\tFeature,\tContinuous<br>\n",
    "A2:\tFeature,\tContinuous<br>\n",
    "A1:\tFeature,\tCategorical<br>\n",
    "\n",
    "Source: https://archive.ics.uci.edu/dataset/27/credit+approval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zTyxuE8gz5Q"
   },
   "source": [
    "1) Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwJIhj4Odjvv"
   },
   "outputs": [],
   "source": [
    "!gdown '1cd6qvO3dSzxmWKXOPMVzL87qIanS8Gqu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKizlkuahC6w"
   },
   "source": [
    "2) Load the required modules: pandas, numpy and matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eM2rpkwJhLrq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGYgfbp9hSRN"
   },
   "source": [
    "3) Read the dataset into a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pmoMkzjZd7XR"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('credit_approval.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3k9cnHHf9WC"
   },
   "source": [
    "4) Examine the number of missing observations per variable, using the DataFrame command isnull.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pTguh4hEdrYf"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFJpF1NWrED2"
   },
   "source": [
    "5) Drop observations with missing values using the DataFrame command .dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-i813dKLen2O"
   },
   "outputs": [],
   "source": [
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bg-z4L64rS2w"
   },
   "source": [
    "6) Display the first 10 rows of your DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oU7jIHlffd4V"
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3GnaGkZrr4k"
   },
   "source": [
    "7) Display the last 10 rows of your DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yLBeTk2XftTt"
   },
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dFodxdwuPnh"
   },
   "source": [
    "8) Create a function named 'credit_approved'. The function should accept a parameter with a value of either + or -. The function should return 1 if the value was a +, 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdaUoNLuuvuX"
   },
   "outputs": [],
   "source": [
    "def credit_approved(x):\n",
    "  if x=='+':\n",
    "    return 1\n",
    "  else:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8c7PQjfvGnb"
   },
   "source": [
    "9) Create a new column in your DataFrame with the name 'approved' by applying the 'credit_approved' function to the column 'approval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bAx0BiJOvesD"
   },
   "outputs": [],
   "source": [
    "df['approved']=df['approval'].apply(credit_approved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jbVRlkYvrx_"
   },
   "source": [
    "10) Drop the column 'approval' from the DataFrame by using the drop(columns='approval') command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "670VSuKCu3bU"
   },
   "outputs": [],
   "source": [
    "df=df.drop(columns='approval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AtswYdYt3pc"
   },
   "source": [
    "11) Use the command .value_counts() on the column 'approval' to compute the number of credit approvals that were approved vs those that were denied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6gy890a1wCj7"
   },
   "outputs": [],
   "source": [
    "df['approved'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7XQp9xqrw4U"
   },
   "source": [
    "12) Create dummy variables from all the categorical variables. You can use the following code:\n",
    "\n",
    "df=pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Loc8DuLurlzl"
   },
   "outputs": [],
   "source": [
    "df=pd.get_dummies(df, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51FtFdzGg5Rd"
   },
   "source": [
    "13) Obtain information about the variables in the dataset using the describe().T command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wuc91KH2ffSh"
   },
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRSS-vl7xPxp"
   },
   "source": [
    "14) Split your DataFrame into two parts:\n",
    "\n",
    "- *X* which contains all the features (explanatory variables)<br>\n",
    "- *y* which is your outcome variable (credit approval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rdyleavex_DA"
   },
   "outputs": [],
   "source": [
    "X=df.drop(columns=['approved'])\n",
    "y=df['approved']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmmd3-lPyCyP"
   },
   "source": [
    "15) Split your X and y datasets into training and test datasets:\n",
    "\n",
    "  Use 85% of your sample for training and 15% of your sample for testing.\n",
    "\n",
    "  You can use the train_test_split function from sklearn.model_selection to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-VokLxbPyEyC"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97u_Z_-3yI5A"
   },
   "source": [
    "16) Normalize the columns in your X vectors. Normalization means that you change the range of each variable from 0 to 1.\n",
    "\n",
    "  You can normalize all values using the following lines of code:\n",
    "\n",
    "  from sklearn import preprocessing<br>\n",
    "  min_max_scaler = preprocessing.MinMaxScaler()<br>\n",
    "  X_train = min_max_scaler.fit_transform(X_train)<br>\n",
    "  X_test = min_max_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DCsVdgVGy19d"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bo5xSgoL_y3D"
   },
   "source": [
    "17) Use the following machine learning algorithms to predict Fraudulent reporting.\n",
    "\n",
    "  1. k-Nearest neighbors\n",
    "  2. Logistic regression\n",
    "  3. Decision tree\n",
    "  4. Neural network\n",
    "\n",
    "For each algorithm save the accuracy, precision and recall scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUHcy4wSAHHH"
   },
   "source": [
    "17a) K-Nearest Neigbors:\n",
    "- Import the k-Nearest neighbors classifier algorithm from scikit-learn.\n",
    "- Import *ConfusionMatrixDisplay*, *precision_score* and *recall_score* from sklearn.metrics\n",
    "- Create an instance of the model and fit it to the training data\n",
    "- Compute: 1) the accuracy of the model on the test data, 2) the precision score for the test data, and 3) the recall score for the test data.\n",
    "- Display a confusion matrix\n",
    "- Decide on the appropriate 'k' for the the algorithm.\n",
    "\n",
    "Utilize print statements with f-strings to neatly display the scores.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPdEybOTAGyX"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn=KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, recall_score, precision_score\n",
    "\n",
    "y_pred=knn.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy score:  {knn.score(X_test,y_test):.4f}\")\n",
    "print(f\"Precision score: {precision_score(y_test,y_pred):.4f}\")\n",
    "print(f\"Recall score:    {recall_score(y_test,y_pred):.4f}\")\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2P_n1bGALka"
   },
   "source": [
    "17b) Logistic regression:\n",
    "- Import the logistic regression classifier algorithm from scikit-learn.\n",
    "- Create an instance of the model and fit it to the training data\n",
    "- Compute: 1) the accuracy of the model on the training data, 2) the accuracy of the model on the test data, 3) the precision score for the test data, and 4) the recall score for the test data.\n",
    "- Display a confusion matrix\n",
    "\n",
    "Utilize print statements with f-strings to neatly display the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVNhyEtLAO-Z"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic=LogisticRegression()\n",
    "logistic.fit(X_train,y_train)\n",
    "\n",
    "y_pred=logistic.predict(X_test)\n",
    "\n",
    "print(f\"Acc. training:   {logistic.score(X_train,y_train):.4f}\")\n",
    "print(f\"Accuracy score:  {logistic.score(X_test,y_test):.4f}\")\n",
    "print(f\"Precision score: {precision_score(y_test,y_pred):.4f}\")\n",
    "print(f\"Recall score:    {recall_score(y_test,y_pred):.4f}\")\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7oM6kvwOASeW"
   },
   "source": [
    "17c) Decision tree:\n",
    "- Import the decision tree classifier algorithm from scikit-learn.\n",
    "- Create an instance of the model and fit it to the training data\n",
    "- Compute: 1) the accuracy of the model on the training data, 2) the accuracy of the model on the test data, 3) the precision score for the test data, and 4) the recall score for the test data.\n",
    "- Display a confusion matrix\n",
    "- Decide on the appropriate depth of the tree.\n",
    "\n",
    "Utilize print statements with f-strings to neatly display the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Yh-cm3YATbN"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier(max_depth=4)\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "y_pred=dtc.predict(X_test)\n",
    "\n",
    "print(f\"Acc. training:   {dtc.score(X_train,y_train):.4f}\")\n",
    "print(f\"Accuracy score:  {dtc.score(X_test,y_test):.4f}\")\n",
    "print(f\"Precision score: {precision_score(y_test,y_pred):.4f}\")\n",
    "print(f\"Recall score:    {recall_score(y_test,y_pred):.4f}\")\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WNdbpvTAT-1"
   },
   "source": [
    "17d) Neural network:\n",
    "- Import the MLP neural network classifier algorithm from scikit-learn.\n",
    "- Create an instance of the model and fit it to the training data\n",
    "- Compute: 1) the accuracy of the model on the training data, 2) the accuracy of the model on the test data, 3) the precision score for the test data, and 4) the recall score for the test data.\n",
    "- Display a confusion matrix\n",
    "\n",
    "Utilize print statements with f-strings to neatly display the scores.\n",
    "\n",
    "Note: you can increase the maximum number of iterations by setting as follows:<br>\n",
    "mlp = MLPClassifier(max_iter=2000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyUvLJpnyhqN"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(max_iter=2000)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "y_pred=mlp.predict(X_test)\n",
    "\n",
    "print(f\"Acc. training:   {mlp.score(X_train,y_train):.4f}\")\n",
    "print(f\"Accuracy score:  {mlp.score(X_test,y_test):.4f}\")\n",
    "print(f\"Precision score: {precision_score(y_test,y_pred):.4f}\")\n",
    "print(f\"Recall score:    {recall_score(y_test,y_pred):.4f}\")\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUI_OLn05XiJ"
   },
   "source": [
    "18) Discuss the algorithms you used and how you determined the optimal specifications. Which algorithm performs best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5-Yl7zW5XqQ"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
